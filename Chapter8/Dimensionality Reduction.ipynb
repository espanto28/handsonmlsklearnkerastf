{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"dim_reduction\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "*The Curse of Dimensionality* imagine if you need to train a model that uses thosands or millions of feature for each instance. This can make training very slow but also very difficult to find a good solution. \n",
    "\n",
    "A example is the MNIST dataset, the pixel on the border are almost always white. So we can remove them because they do not help us with the training. However we are removing information. There is a trade off for speed with information loss which might result in a slightly worse model. \n",
    "\n",
    "The curse of dimensionality is when each training distance is so far from another due to the high dimensions of space. If you have a dataset with 1 million feature and 2 million instances. It is unlikely that instance 1 will be close to instance 200,000. \n",
    "\n",
    "This means there is a great chance of overfitting. \n",
    "\n",
    "## Main Approaches for Dimensinality Reduction\n",
    "\n",
    "### Projection\n",
    "\n",
    "Projection is when we tak eevery training instnace and we project it in another dimension. Like a 3D dataset and move it to a 2D. Why would we do this well just like the MNIST dataset we know that not all features are important. \n",
    "\n",
    "Lets see the below graph as a way to represent this\n",
    "\n",
    "<img src=\"images/3d_2d.png\"/> </img>\n",
    "\n",
    "We can see we take a 3D images but select specific features that are important to us and predict. Notice how we do not select any instances in the white area since they are not useful points.\n",
    "\n",
    "### Manifold Learning\n",
    "\n",
    "This is when we seek to understand the a 2D dataset even when it is contorted into a 3D dataset. It is a 2D shape that can be twisted into a higher dimensional shape. \n",
    "\n",
    "<img src=\"images/swiss_roll.png\"/> </img>\n",
    "\n",
    "This relies on the Manifold assumption that is also called manifold hypothesis  which states that most real wold high dimensional datasets lie close to a much lower dimensional manifold. This is often accompanied by a another assumption that the task at hand is often simpler if expressed in the lower diimensional space of the manifold. \n",
    "\n",
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "One of the most popular algorithms for dimensinality reduction algorithm. First it identities the hyperplane that lis closest to the data and then it projecs the data onto it. \n",
    "\n",
    "In one sentence it identifies the hyperplane that lies cloest to the data and then it projects the data onto it.\n",
    "\n",
    "The first thing that needs to happen is picking the right hyperplane. We usually want to pick up a plane with the maximum variance as shown on the figure below:\n",
    "\n",
    "<img src=\"images/pca_maximum_variance.jpeg \"/></img>\n",
    "\n",
    "Notice how we select the PC1 because it has the maximum variance or range of different values. \n",
    "\n",
    "The ith axis is the ith principal component of the data. The third line is usually the axis orthogonal to that plane. \n",
    "\n",
    "One way to find the components of a traiing set is Singular Value Decomposition (SVD). \n",
    "\n",
    "What SVD does is decompose the training set matrix X into the matrix mutlicplication of 3 matrices. Where Vt contains the unit vectors that defin ehe principal components that we are looking for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a 3D dataset\n",
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m, 3))\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVD\n",
    "X_centered = X - X.mean(axis=0)\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "c1 = Vt.T[:, 0]\n",
    "c2 = Vt.T[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are doing above is making sure we center the line. But without changinging the relative points. This video has a great explanation\n",
    "\n",
    "https://www.youtube.com/watch?v=FgakZw6K1QQ&ab_channel=StatQuestwithJoshStarmer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = X.shape\n",
    "\n",
    "S = np.zeros(X_centered.shape)\n",
    "S[:n, :n] = np.diag(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(X_centered, U.dot(S).dot(Vt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = Vt.T[:, :2]\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2D_using_svd = X2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.690074  , -0.36150744],\n",
       "       [ 1.39636097,  0.34497714],\n",
       "       [ 1.00728461, -0.35025708],\n",
       "       [ 0.2736333 , -0.50516373],\n",
       "       [-0.91324535,  0.26290852],\n",
       "       [-0.79710027,  0.26718188],\n",
       "       [-0.55173939,  0.65062721],\n",
       "       [ 1.41612959,  0.16567641],\n",
       "       [ 0.40776059, -0.46053322],\n",
       "       [ 0.85209856, -0.40516935],\n",
       "       [-0.46269946,  0.61952736],\n",
       "       [ 1.2826692 ,  0.41018903],\n",
       "       [ 1.37468032,  0.03618608],\n",
       "       [-0.96941594,  0.10932241],\n",
       "       [-0.97219266,  0.14390464],\n",
       "       [-1.05216924,  0.07740862],\n",
       "       [-0.92770444, -0.22364286],\n",
       "       [-0.01473543, -0.4153169 ],\n",
       "       [-0.47010859, -0.46920058],\n",
       "       [-0.87761843, -0.08515546],\n",
       "       [ 0.38973612, -0.45189716],\n",
       "       [-0.96989867,  0.19819051],\n",
       "       [-0.93689997, -0.09307933],\n",
       "       [-0.81304146, -0.26096051],\n",
       "       [-0.41368569, -0.42009096],\n",
       "       [ 1.2830484 , -0.02603822],\n",
       "       [-0.95210787,  0.23163682],\n",
       "       [-0.2005476 , -0.49130242],\n",
       "       [ 0.33988682, -0.41263482],\n",
       "       [-0.59146086,  0.58475112],\n",
       "       [ 0.54812172, -0.43831338],\n",
       "       [-0.99520427,  0.14358061],\n",
       "       [-0.58645755,  0.57426772],\n",
       "       [ 1.39360363,  0.3895672 ],\n",
       "       [ 1.33617687,  0.38561674],\n",
       "       [ 1.41628081,  0.04632763],\n",
       "       [-0.9264541 , -0.18501519],\n",
       "       [-0.77991339,  0.34821204],\n",
       "       [ 0.74686904, -0.26218164],\n",
       "       [-0.48042341, -0.47513888],\n",
       "       [-0.75151549,  0.37787401],\n",
       "       [-0.18283935, -0.49263035],\n",
       "       [-0.51056407,  0.63475101],\n",
       "       [ 1.39723764,  0.32572316],\n",
       "       [-0.97100408, -0.08408757],\n",
       "       [ 0.60807764, -0.49285352],\n",
       "       [-0.93297154, -0.20657396],\n",
       "       [-0.15026573, -0.52219942],\n",
       "       [ 0.09681863, -0.4867323 ],\n",
       "       [-0.94691677,  0.14421944],\n",
       "       [ 1.36756046,  0.42677217],\n",
       "       [ 1.25473742, -0.12194352],\n",
       "       [ 1.3577182 ,  0.29036654],\n",
       "       [ 1.38554792,  0.21587043],\n",
       "       [ 0.38277905, -0.37983393],\n",
       "       [ 1.39874611,  0.2685979 ],\n",
       "       [-0.65765024,  0.40980972],\n",
       "       [-0.92060108,  0.1583512 ],\n",
       "       [-0.53772935,  0.47010862],\n",
       "       [-0.8066079 , -0.13305015]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2D_using_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scikit-Learn\n",
    "\n",
    "SKlearn has the ability to use SVD decomposition to implement PCA. The following code will reduce the dataset down to two dimensions and it takes care of centering the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.690074  , -0.36150744],\n",
       "       [ 1.39636097,  0.34497714],\n",
       "       [ 1.00728461, -0.35025708],\n",
       "       [ 0.2736333 , -0.50516373],\n",
       "       [-0.91324535,  0.26290852],\n",
       "       [-0.79710027,  0.26718188],\n",
       "       [-0.55173939,  0.65062721],\n",
       "       [ 1.41612959,  0.16567641],\n",
       "       [ 0.40776059, -0.46053322],\n",
       "       [ 0.85209856, -0.40516935],\n",
       "       [-0.46269946,  0.61952736],\n",
       "       [ 1.2826692 ,  0.41018903],\n",
       "       [ 1.37468032,  0.03618608],\n",
       "       [-0.96941594,  0.10932241],\n",
       "       [-0.97219266,  0.14390464],\n",
       "       [-1.05216924,  0.07740862],\n",
       "       [-0.92770444, -0.22364286],\n",
       "       [-0.01473543, -0.4153169 ],\n",
       "       [-0.47010859, -0.46920058],\n",
       "       [-0.87761843, -0.08515546],\n",
       "       [ 0.38973612, -0.45189716],\n",
       "       [-0.96989867,  0.19819051],\n",
       "       [-0.93689997, -0.09307933],\n",
       "       [-0.81304146, -0.26096051],\n",
       "       [-0.41368569, -0.42009096],\n",
       "       [ 1.2830484 , -0.02603822],\n",
       "       [-0.95210787,  0.23163682],\n",
       "       [-0.2005476 , -0.49130242],\n",
       "       [ 0.33988682, -0.41263482],\n",
       "       [-0.59146086,  0.58475112],\n",
       "       [ 0.54812172, -0.43831338],\n",
       "       [-0.99520427,  0.14358061],\n",
       "       [-0.58645755,  0.57426772],\n",
       "       [ 1.39360363,  0.3895672 ],\n",
       "       [ 1.33617687,  0.38561674],\n",
       "       [ 1.41628081,  0.04632763],\n",
       "       [-0.9264541 , -0.18501519],\n",
       "       [-0.77991339,  0.34821204],\n",
       "       [ 0.74686904, -0.26218164],\n",
       "       [-0.48042341, -0.47513888],\n",
       "       [-0.75151549,  0.37787401],\n",
       "       [-0.18283935, -0.49263035],\n",
       "       [-0.51056407,  0.63475101],\n",
       "       [ 1.39723764,  0.32572316],\n",
       "       [-0.97100408, -0.08408757],\n",
       "       [ 0.60807764, -0.49285352],\n",
       "       [-0.93297154, -0.20657396],\n",
       "       [-0.15026573, -0.52219942],\n",
       "       [ 0.09681863, -0.4867323 ],\n",
       "       [-0.94691677,  0.14421944],\n",
       "       [ 1.36756046,  0.42677217],\n",
       "       [ 1.25473742, -0.12194352],\n",
       "       [ 1.3577182 ,  0.29036654],\n",
       "       [ 1.38554792,  0.21587043],\n",
       "       [ 0.38277905, -0.37983393],\n",
       "       [ 1.39874611,  0.2685979 ],\n",
       "       [-0.65765024,  0.40980972],\n",
       "       [-0.92060108,  0.1583512 ],\n",
       "       [-0.53772935,  0.47010862],\n",
       "       [-0.8066079 , -0.13305015]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85406025, 0.13622918])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printing out explained variance ratio\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
