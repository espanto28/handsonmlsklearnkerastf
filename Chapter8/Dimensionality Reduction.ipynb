{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "*The Curse of Dimensionality* imagine if you need to train a model that uses thosands or millions of feature for each instance. This can make training very slow but also very difficult to find a good solution. \n",
    "\n",
    "A example is the MNIST dataset, the pixel on the border are almost always white. So we can remove them because they do not help us with the training. However we are removing information. There is a trade off for speed with information loss which might result in a slightly worse model. \n",
    "\n",
    "The curse of dimensionality is when each training distance is so far from another due to the high dimensions of space. If you have a dataset with 1 million feature and 2 million instances. It is unlikely that instance 1 will be close to instance 200,000. \n",
    "\n",
    "This means there is a great chance of overfitting. \n",
    "\n",
    "## Main Approaches for Dimensinality Reduction\n",
    "\n",
    "### Projection\n",
    "\n",
    "Projection is when we tak eevery training instnace and we project it in another dimension. Like a 3D dataset and move it to a 2D. Why would we do this well just like the MNIST dataset we know that not all features are important. \n",
    "\n",
    "Lets see the below graph as a way to represent this\n",
    "\n",
    "<img src=\"images/3d_2d.png\"/> </img>\n",
    "\n",
    "We can see we take a 3D images but select specific features that are important to us and predict. Notice how we do not select any instances in the white area since they are not useful points.\n",
    "\n",
    "### Manifold Learning\n",
    "\n",
    "This is when we seek to understand the a 2D dataset even when it is contorted into a 3D dataset. It is a 2D shape that can be twisted into a higher dimensional shape. \n",
    "\n",
    "<img src=\"images/swiss_roll.png\"/> </img>\n",
    "\n",
    "This relies on the Manifold assumption that is also called manifold hypothesis  which states that most real wold high dimensional datasets lie close to a much lower dimensional manifold. This is often accompanied by a another assumption that the task at hand is often simpler if expressed in the lower diimensional space of the manifold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
